"""
ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤
"""

import sys
import io
from pathlib import Path

# UTF-8 ì¶œë ¥ ì„¤ì •
sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding="utf-8")

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ ê²½ë¡œì— ì¶”ê°€
sys.path.insert(0, str(Path(__file__).parent.parent))

from jaso_jamo import tokenize, detokenize


def test_basic_words():
    """ê¸°ë³¸ ë‹¨ì–´ í…ŒìŠ¤íŠ¸"""
    test_cases = [
        "í•œê¸€",
        "ì•ˆë…•",
        "ê°ì‚¬",
        "ì‚¬ë‘",
        "í–‰ë³µ",
    ]

    print("=" * 60)
    print("ê¸°ë³¸ ë‹¨ì–´ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    passed = 0
    for text in test_cases:
        tokens = tokenize(text)
        restored = detokenize(tokens)
        status = "v" if text == restored else "âŒ"
        print(f"{status} '{text}' â†’ {tokens} â†’ '{restored}'")
        if text == restored:
            passed += 1

    print(f"\ní†µê³¼: {passed}/{len(test_cases)}\n")
    return passed == len(test_cases)


def test_jongseong():
    """ì¢…ì„± í…ŒìŠ¤íŠ¸"""
    test_cases = [
        "ê°",
        "ê°„",
        "ê°ˆ",
        "ê°",
        "ê°‘",
        "ê°“",
        "ê°•",
        "êµ­",
        "êµ³",
        "êµ´",
        "êµ¼",
        "êµ½",
        "ë°",
        "ë‹­",
        "ì‚¶",
        "ì•",
        "ì˜†",
    ]

    print("=" * 60)
    print("ì¢…ì„± í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    passed = 0
    for text in test_cases:
        tokens = tokenize(text)
        restored = detokenize(tokens)
        status = "v" if text == restored else "âŒ"
        print(f"{status} '{text}' â†’ {tokens} â†’ '{restored}'")
        if text == restored:
            passed += 1

    print(f"\ní†µê³¼: {passed}/{len(test_cases)}\n")
    return passed == len(test_cases)


def test_long_sentences():
    """ê¸´ ë¬¸ì¥ í…ŒìŠ¤íŠ¸"""
    test_cases = [
        "ì•ˆë…•í•˜ì„¸ìš”",
        "ê°ì‚¬í•©ë‹ˆë‹¤",
        "ë°˜ê°‘ìŠµë‹ˆë‹¤",
        "í•œê¸€ ìì†Œ ë¶„ë¦¬ì™€ ë³µì›",
        "ìì—°ì–´ ì²˜ë¦¬ ë¼ì´ë¸ŒëŸ¬ë¦¬",
        "5ë‹¨ê³„ Fallback ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•˜ì˜€ìŠµë‹ˆë‹¤",
    ]

    print("=" * 60)
    print("ê¸´ ë¬¸ì¥ í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    passed = 0
    for text in test_cases:
        tokens = tokenize(text)
        restored = detokenize(tokens)
        status = "v" if text == restored else "âŒ"
        print(f"{status} '{text[:30]}...' â†’ '{restored[:30]}...'")
        if text == restored:
            passed += 1

    print(f"\ní†µê³¼: {passed}/{len(test_cases)}\n")
    return passed == len(test_cases)


def main():
    """ëª¨ë“  í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
    print("\n" + "=" * 60)
    print("í•œê¸€ ìì†Œ ë³µì› ê¸°ë³¸ í…ŒìŠ¤íŠ¸")
    print("=" * 60 + "\n")

    results = []
    results.append(("ê¸°ë³¸ ë‹¨ì–´", test_basic_words()))
    results.append(("ì¢…ì„±", test_jongseong()))
    results.append(("ê¸´ ë¬¸ì¥", test_long_sentences()))

    print("=" * 60)
    print("ì „ì²´ ê²°ê³¼")
    print("=" * 60)

    for name, passed in results:
        status = "v PASS" if passed else "âŒ FAIL"
        print(f"{status} - {name}")

    all_passed = all(r[1] for r in results)
    print("\n" + ("ğŸ‰ ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼!" if all_passed else "âš ï¸ ì¼ë¶€ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨"))
    print("=" * 60 + "\n")


if __name__ == "__main__":
    main()
